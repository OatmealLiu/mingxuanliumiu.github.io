<!DOCTYPE html>
<html>
  <head>
    <title>Survey on Few-Shot and Self-Supervised Object Detection : Project Page</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Nunito:wght@400;600&display=swap" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Nunito:wght@400;600&family=PT+Sans:wght@400;700&display=swap" rel="stylesheet">      <style>
      body {
        font-family: 'PT Sans', sans-serif;
      }
      a {
        text-decoration: none;
        color: #0969da;
      }
      a.cute {
        
      }
      #paper-container {
        text-align:center;
      }
      #paper-head {
        width: 800px;
        margin: auto;
      }
      video.tutorial, img.tutorial {
        width: 70%;
        margin: auto;
        display: block;
      }
      h1, h2 {
        margin-top: 5ex;
      }
      li {
        margin-top: 4ex;
      }
      pre code {
        background-color: #eee;
        border: 1px solid #999;
        display: block;
        padding: 5px;
        overflow: scroll;
      }
    </style>
  </head>
  <body>
    <div id="paper-container">
      <div id="paper-head">
        ðŸ”™ <a href="https://gabrielhuang.github.io/">Back to Gabriel Huang's home page</a>
        <a href="https://arxiv.org/abs/2110.14711"><h1>A Survey on Few-Shot and <br>Self-Supervised Object Detection</h1></a>
        <p>
          <a href="https://gabrielhuang.github.io/">Gabriel Huang</a>, 
          <a href="https://issamlaradji.github.io/">Issam Laradji</a>,
          <a href="http://www.david-vazquez.com/">David Vazquez</a>,
          <a href="http://www.iro.umontreal.ca/~slacoste/">Simon Lacoste-Julien</a>,
          <a href="https://prlz77.github.io/resume/">Pau Rodriguez</a>
        </p>
        <a href="https://arxiv.org/pdf/2110.14711.pdf#page=2"><img width="640" src="few-shot-and-self-supervised-object-detection-taxonomy.jpg"></img></a>

        <p>
          [<a class="cute" href="https://arxiv.org/abs/2110.14711">arXiv</a>]  
          [<a class="cute" href="https://arxiv.org/pdf/2110.14711.pdf">PDF</a>]
          [<a class="cute" href="https://github.com/gabrielhuang/awesome-few-shot-object-detection">Awesome-FSOD</a>]
        </p>

        <div style="text-align: justify;">


        <h2>Abstract</h2></a>
        <p style="text-align: justify;">
          Labeling data is often expensive and time-consuming, 
          especially for tasks such as object detection and instance segmentation, 
          which require dense labeling of the image. 
          While few-shot object detection is about training a model on 
          novel (unseen) object classes with little data, 
          it still requires prior training on many labeled examples of base (seen) classes. 
          On the other hand, self-supervised methods aim at learning representations 
          from unlabeled data which transfer well to downstream tasks such as object detection. 
          Combining <a href="https://arxiv.org/pdf/2110.14711.pdf#page=4">few-shot</a> and <a href="https://arxiv.org/pdf/2110.14711.pdf#page=11">self-supervised</a> object detection is a promising research direction. 
          In this survey, we review and characterize the most recent 
          approaches on few-shot and self-supervised object detection. 
          Then, we give our main <a href="https://arxiv.org/pdf/2110.14711.pdf#page=15">takeaways</a> and discuss future research directions.
        </p>

        <h2>Tweetorial</h2>
        <!-- Also: read and comment on <a href="">Twitter</a> -->
          Our <i>tweetorial</i> is summarized below for your convenience. Click on paragraph titles or images for quick links to relevant paper section.
          <ol style>
            <li><b><a href="https://arxiv.org/pdf/2110.14711.pdf#page=15">The current disorganization of self-supervised 
              and few-shot object detection</a></b> have led to *two* Meta-RCNN papers ðŸ˜±! 
              Also, did you know that you can boost your 1-shot Pascal performance by 57% simply by changing your evaluation procedure?
              Here we propose a survey to fix it!
              Bonus: we also provide lots of interesting insights and future directions.<br>
              <a href="https://arxiv.org/pdf/2110.14711.pdf#page=2"><img class="tutorial" src="few-shot-and-self-supervised-object-detection-taxonomy.jpg"></img></a>
            </li>
            <li><a href="https://arxiv.org/pdf/2110.14711.pdf#page=15"><b>Finetuning is a strong baseline</a></b>. Despite increasingly sophisticated conditioning-based methods being proposed, fine-tuning remains a very strong baseline (TFA, MPSR, DETReg). In fact, most if not all conditioning-based methods perform better when finetuned.<br>
              <a href="https://arxiv.org/pdf/2110.14711.pdf#page=15"><img class="tutorial" src="finetuning-meme.jpeg"></img></a>
            </li>
            <li>
              <b><a href="https://arxiv.org/pdf/2110.14711.pdf#page=15">Heuristics can generate weak labels</a></b>. Several self-supervised methods now rely on training-free computer vision heuristics as training signal. DETReg and SoCo use Selective Search to generate object crops; DETCon compare several heuristics for feature pooling (below)
              <a href="https://arxiv.org/pdf/2110.14711.pdf#page=15"><img class="tutorial" src="detcon.jpg"></img></a>
            </li>
            <li>
              <b><a href="https://arxiv.org/pdf/2110.14711.pdf#page=15">Transformers are everywhere</a></b>. Transformer detection heads such as DETR are increasingly popular due to their simplicity and ability to be trained end-to-end. Moreover, DINO showed that visual transformer backbones can learn semantic segmentation maps without any supervision.
              <video class="tutorial" controls Autoplay=autoplay>
                <source src="dino.mov" type="video/mp4">
                <img src="dino.jpg">
              </video>
            </li>
            <li>
              <b><a href="https://arxiv.org/pdf/2110.14711.pdf#page=9">LEARN HOW I INCREASED MY PASCAL 1-SHOT PERFORMANCE BY 57% WITHOUT DOING ANY WORK!</a></b> Thatâ€™s right, this is how much discrepancy between benchmarking on Kangâ€™s splits and TFAâ€™s split. In the future, please report those numbers in *separate* columns like below! Also, why does everyone keep using Pascal & COCO trainval as training set and test as validation, instead of using train/val/test splits properly? Find our recommended best practices in the paper!
              <a href="https://arxiv.org/pdf/2110.14711.pdf#page=9"><img class="tutorial" src="fsod-table.png"></img></a>
            </li>
            <li>
              <b><a href="https://arxiv.org/pdf/2110.14711.pdf#page=5">Few-shot classification vs detection</a></b>. Coming from few-shot *classification* and being used to the N-way K-shot episodic framework, I was very confused at first with the few-shot *object detection* framework, which in many ways is closer to transfer learning. We explain the differences between the two in-depth.
              <a href="https://arxiv.org/pdf/2110.14711.pdf#page=5"><img class="tutorial" src="fsod-framework.png" style="width: 50%;"></img></a>
            </li>
            <li>
              <b><a href="https://arxiv.org/pdf/2110.14711.pdf#page=8">Do you actually know how mAP is computed?</a></b> Evaluating object detectors &mdash;let alone few-shot object detectors&mdash; is a nontrivial task. Check out our survey to learn the difference between Pascal VOC and COCO-style mean average precision and how to compute them!
              <a href="https://arxiv.org/pdf/2110.14711.pdf#page=8"><img class="tutorial" src="precision-recall.png" style="width: 50%;"></img></a>
            </li>
            <li>
              <b><a href="https://arxiv.org/pdf/2110.14711.pdf#page=14">Self-supervision *for* object detection</a></b>. Beyond initializing the backbone with MoCo/SimCLR/SwaV, several works now attempt to pretrain the detection heads as well. This seems most useful on few-shot and low-data (e.g. 1%, 5%, 10% COCO) scenarios. Currently one of the top methods, DETReg uses self-supervised representations for few-shot object detection.
              <a href="https://arxiv.org/pdf/2110.14711.pdf#page=14"><img class="tutorial" src="ssod-table.png" style="width: 50%;"></img></a>
            </li>
            <li>
              <b><a href="https://arxiv.org/pdf/2110.14711.pdf#page=16">Related tasks</a></b>. Letâ€™s keep a close eye on the related work: zero-shot object detection, weakly-supervised object detection, (few-shot) semantic and instance segmentation, visual commonsense/question answering/captioning, (few-shot) video detection and tracking, multimodal approaches, and more!
              <!-- https://github.com/JunweiLiang/Object_Detection_Tracking -->
              <a href="https://arxiv.org/pdf/2110.14711.pdf#page=16"><img class="tutorial" src="object-tracking.gif"></img></a>
            </li>
          </ol>

          <h2>Read on Twitter</h2>
<blockquote class="twitter-tweet" data-theme="light"><p lang="en" dir="ltr">1/9. The current disorganization of self-supervised and few-shot object detection have led to *two* Meta-RCNN papers ðŸ˜±! Here we propose a survey to fix it! Bonus: we also provide lots of interesting insights and future directions. <a href="https://t.co/aw935vimD2">https://t.co/aw935vimD2</a> <a href="https://t.co/xlrhammDIu">pic.twitter.com/xlrhammDIu</a></p>&mdash; Gabriel Huang (@GabrielHuang9) <a href="https://twitter.com/GabrielHuang9/status/1455639917497827334?ref_src=twsrc%5Etfw">November 2, 2021</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

          <h2>Citation</h2>
            If our paper was useful to you, please cite us!
            <pre><code>@article{huang2021survey,
  title={A Survey of Self-Supervised and Few-Shot Object Detection},
  author={Huang, Gabriel and Laradji, Issam and Vazquez, David and Lacoste-Julien, Simon and Rodriguez, Pau},
  journal={arXiv preprint arXiv:2110.14711},
  year={2021}
}</code></pre>
          </div>
      </div>
    </div>
  </body>
</html>
